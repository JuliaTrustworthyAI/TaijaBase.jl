module Samplers

using CategoricalArrays
using Distributions
using Flux
using Flux.Optimise: apply!, Optimiser
using MLUtils
using Optimisers
using Tables

"Base type for sampling rules."
abstract type AbstractSamplingRule <: Optimisers.AbstractRule end

"Base type for samplers."
abstract type AbstractSampler end

export AbstractSampler, AbstractSamplingRule
export ConditionalSampler, UnconditionalSampler, JointSampler
export PCD
export energy

include("utils.jl")
include("optimizers.jl")

"""
    (sampler::AbstractSampler)(
        model,
        rule::AbstractSamplingRule;
        niter::Int = 100,
        clip_grads::Union{Nothing,AbstractFloat} = 1e-2,
        n_samples::Union{Nothing,Int} = nothing,
        kwargs...,
    )

Base method for generating Monte Carlo samples for a given models, sampler and sampling rule.
"""
function (sampler::AbstractSampler)(
    model,
    rule::AbstractSamplingRule;
    niter::Int = 100,
    clip_grads::Union{Nothing,AbstractFloat} = 1e-2,
    n_samples::Union{Nothing,Int} = nothing,
    kwargs...,
)

    n_samples = isnothing(n_samples) ? sampler.batch_size : n_samples

    # Initialize chain:
    reinit = rand(Binomial(1, (1.0 - sampler.prob_buffer)))
    if reinit == 1
        # Initialize chain from random samples:
        input_samples = Float32.(rand(sampler.ð’Ÿx, sampler.input_size..., n_samples))
    else
        # Initialize chain from buffer:
        input_samples = selectdim(
            sampler.buffer,
            ndims(sampler.buffer),
            rand(1:size(sampler.buffer, ndims(sampler.buffer)), n_samples),
        )
    end

    # Perform MCMC sampling:
    rule =
        isnothing(clip_grads) ? rule :
        Optimisers.OptimiserChain(Optimisers.ClipGrad(clip_grads), rule)
    Flux.testmode!(model)
    input_samples = mcmc_samples(sampler, model, rule, input_samples; niter = niter, kwargs...)
    Flux.trainmode!(model)
    input_samples = Float32.(clamp.(input_samples, minimum(sampler.ð’Ÿx), maximum(sampler.ð’Ÿx)))

    # Update buffer:
    sampler.buffer = cat(input_samples, sampler.buffer, dims = ndims(sampler.buffer))
    _end = minimum([size(sampler.buffer, ndims(sampler.buffer)), sampler.max_len])
    sampler.buffer = selectdim(sampler.buffer, ndims(sampler.buffer), 1:_end)

    return input_samples

end

"""
    PCD(
        sampler::AbstractSampler,
        model,
        rule::AbstractSamplingRule;
        ntransitions::Int = 100,
        niter::Int = 100,
        kwargs...,
    )

Persistent Contrastive Divergence (PCD) algorithm. This algorithm was originally proposed by [Tieleman (2008)](https://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf) and is a variant of the Contrastive Divergence (CD) algorithm. The main difference is that PCD uses a persistent chain to estimate the negative phase of the gradient. This is done by keeping the state of the Markov chain between iterations. 

In our context, the sampler is the persistent chain and the model is a supervised model. The sampler generates samples from the model's learned distribution. 

# Note

This function does not perform any training. It only generates samples from the model. For training Joint Energy Models, see [JointEnergyModels.jl](https://github.com/JuliaTrustworthyAI/JointEnergyModels.jl).

# Arguments

- `sampler::AbstractSampler`: The sampler to use.
- `model`: The model to sample from.
- `rule::AbstractSamplingRule`: The sampling rule to use.
- `ntransitions::Int=100`: The number of transitions to perform.
- `niter::Int=100`: The number of iterations to perform.
- `kwargs...`: Additional keyword arguments.

# Returns

- `sampler.buffer`: The buffer containing the samples generated by the sampler.
"""
function PCD(
    sampler::AbstractSampler,
    model,
    rule::AbstractSamplingRule;
    ntransitions::Int = 100,
    niter::Int = 100,
    kwargs...,
)
    i = 1
    while i <= ntransitions
        sampler(model, rule; niter = niter, kwargs...)
        i += 1
    end
    return sampler.buffer
end

@doc raw"""
    ConditionalSampler <: AbstractSampler

Generates conditional samples: $x \sim p(x|y).$
"""
mutable struct ConditionalSampler <: AbstractSampler
    ð’Ÿx::Distribution
    ð’Ÿy::Distribution
    input_size::Dims
    batch_size::Int
    buffer::AbstractArray
    max_len::Int
    prob_buffer::AbstractFloat
end

"""
    ConditionalSampler(
        ð’Ÿx::Distribution, ð’Ÿy::Distribution;
        input_size::Dims, batch_size::Int,
        max_len::Int=10000, prob_buffer::AbstractFloat=0.95
    )

Outer constructor for `ConditionalSampler`.
"""
function ConditionalSampler(
    ð’Ÿx::Distribution,
    ð’Ÿy::Distribution;
    input_size::Dims,
    batch_size::Int = 1,
    max_len::Int = 10000,
    prob_buffer::AbstractFloat = 0.95,
)
    @assert batch_size <= max_len "batch_size must be <= max_len"
    buffer = Float32.(rand(ð’Ÿx, input_size..., maximum([1000, batch_size])))
    return ConditionalSampler(ð’Ÿx, ð’Ÿy, input_size, batch_size, buffer, max_len, prob_buffer)
end

"""
    energy(sampler::ConditionalSampler, model, x, y)

Energy function for `ConditionalSampler`.
"""
function energy(sampler::ConditionalSampler, model, x, y; agg = mean)
    return _energy(model, x, y; agg = agg)
end

"""
    mcmc_samples(
        sampler::ConditionalSampler,
        model,
        rule::Optimisers.AbstractRule,
        input_samples::AbstractArray;
        niter::Int,
        y::Union{Nothing,Int} = nothing,
    )

Sampling method for `ConditionalSampler`.
"""
function mcmc_samples(
    sampler::ConditionalSampler,
    model,
    rule::Optimisers.AbstractRule,
    input_samples::AbstractArray;
    niter::Int,
    y::Union{Nothing,Int} = nothing,
)
    # Setup
    if isnothing(y)
        y = rand(sampler.ð’Ÿy)
    end
    mod = (inputs = input_samples, energy = energy)
    s = Optimisers.setup(rule, mod)

    # Training:
    i = 1
    while i <= niter
        grad = gradient(mod) do m  # calculate the gradients
            m.energy(sampler, model, m.inputs, y)
        end
        s, mod = Optimisers.update(s, mod, grad[1])
        i += 1
    end

    return mod.inputs
end

@doc raw"""
    UnonditionalSampler <: AbstractSampler

Generates unconditional samples: $x \sim p(x).$
"""
mutable struct UnconditionalSampler <: AbstractSampler
    ð’Ÿx::Distribution
    ð’Ÿy::Union{Distribution,Nothing}
    input_size::Dims
    batch_size::Int
    buffer::AbstractArray
    max_len::Int
    prob_buffer::AbstractFloat
end

"""
    UnconditionalSampler(
        ð’Ÿx::Distribution,
        ð’Ÿy::Union{Distribution,Nothing};
        input_size::Dims,
        batch_size::Int = 1,
        max_len::Int = 10000,
        prob_buffer::AbstractFloat = 0.95,
    )

Outer constructor for `UnonditionalSampler`.
"""
function UnconditionalSampler(
    ð’Ÿx::Distribution,
    ð’Ÿy::Union{Distribution,Nothing};
    input_size::Dims,
    batch_size::Int = 1,
    max_len::Int = 10000,
    prob_buffer::AbstractFloat = 0.95,
)
    @assert batch_size <= max_len "batch_size must be <= max_len"
    buffer = Float32.(rand(ð’Ÿx, input_size..., maximum([1000, batch_size])))
    return UnconditionalSampler(
        ð’Ÿx,
        ð’Ÿy,
        input_size,
        batch_size,
        buffer,
        max_len,
        prob_buffer,
    )
end

"""
    energy(sampler::UnconditionalSampler, model, x, y)

Energy function for `UnconditionalSampler`.
"""
function energy(sampler::UnconditionalSampler, model, x, y)
    return _energy(model, x; agg = mean)
end

"""
    mcmc_samples(
        sampler::UnconditionalSampler,
        model,
        rule::Optimisers.AbstractRule,
        input_samples::AbstractArray;
        niter::Int,
        y::Union{Nothing,Int} = nothing,
    )

Sampling method for `UnconditionalSampler`.
"""
function mcmc_samples(
    sampler::UnconditionalSampler,
    model,
    rule::Optimisers.AbstractRule,
    input_samples::AbstractArray;
    niter::Int,
    y::Union{Nothing,Int} = nothing,
)

    # Setup:
    mod = (inputs = input_samples, energy = energy)
    s = Optimisers.setup(rule, mod)

    # Training:
    i = 1
    while i <= niter
        grad = gradient(mod) do m  # calculate the gradients
            m.energy(sampler, model, m.inputs, nothing)
        end
        s, mod = Optimisers.update(s, mod, grad[1])
        i += 1
    end

    return mod.inputs

end

@doc raw"""
    JointSampler <: AbstractSampler

Generates unconditional samples by drawing directly from joint distribution: $x \sim p(x, y).$
"""
mutable struct JointSampler <: AbstractSampler
    ð’Ÿx::Distribution
    ð’Ÿy::Distribution
    input_size::Dims
    batch_size::Int
    buffer::AbstractArray
    max_len::Int
    prob_buffer::AbstractFloat
end

"""
    JointSampler(
        ð’Ÿx::Distribution, ð’Ÿy::Distribution, input_size::Dims, batch_size::Int;
        max_len::Int=10000, prob_buffer::AbstractFloat=0.95
    )

Outer constructor for `JointSampler`.
"""
function JointSampler(
    ð’Ÿx::Distribution,
    ð’Ÿy::Distribution;
    input_size::Dims,
    batch_size::Int = 1,
    max_len::Int = 10000,
    prob_buffer::AbstractFloat = 0.95,
)
    @assert batch_size <= max_len "batch_size must be <= max_len"
    buffer = Float32.(rand(ð’Ÿx, input_size..., maximum([1000, batch_size])))
    return JointSampler(ð’Ÿx, ð’Ÿy, input_size, batch_size, buffer, max_len, prob_buffer)
end

"""
    energy(sampler::JointSampler, model, x, y)

Energy function for `JointSampler`.
"""
function energy(sampler::JointSampler, model, x, y)
    return _energy(model, x, y)
end

"""
    mcmc_samples(
        sampler::JointSampler,
        model,
        rule::Optimisers.AbstractRule,
        input_samples::AbstractArray;
        niter::Int,
        y::Union{Nothing,Int} = nothing,
    )

Sampling method for `JointSampler`.
"""
function mcmc_samples(
    sampler::JointSampler,
    model,
    rule::Optimisers.AbstractRule,
    input_samples::AbstractArray;
    niter::Int,
    y::Union{Nothing,Int} = nothing,
)

    # Setup:
    mod = (inputs = input_samples, energy = energy)
    s = Optimisers.setup(rule, mod)

    # Training:
    i = 1
    while i <= niter
        y = rand(sampler.ð’Ÿy)
        grad = gradient(mod) do m  # calculate the gradients
            m.energy(sampler, model, m.inputs, y)
        end
        s, mod = Optimisers.update(s, mod, grad[1])
        i += 1
    end

    return mod.inputs

end

end
